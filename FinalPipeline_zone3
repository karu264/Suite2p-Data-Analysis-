# ============================================
# Master: stimulus-aligned orientation analysis
# - trials.csv alignment (auto relative/absolute ms)
# - best cells (top-4 per-cell pipeline; top-5 tuning with von Mises + spline)
# - RAW-F raster (no smoothing) + overlays
# - ordered heatmaps (signals CSV + suite2p), z-scored per neuron
# - pref-ori & OSI maps (white bg; size = amplitude; strong outlines)
# - population OSI/angle histograms
# - stats writer -> <output_dir>/stats/stats.txt
# ============================================

import os
from pathlib import Path
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patheffects as pe
from datetime import datetime, timezone, timedelta
import zoneinfo

from scipy.optimize import curve_fit
from scipy.special import i0
from scipy.interpolate import make_interp_spline
from scipy.stats import zscore

# ----------------------===== USER INPUTS =====----------------------
# (DONT FORGET TO CHANGE per session!!!)
# CSV with Y columns== need to be created in the following block
# Stimulus log CSV
stimlog_csv_path = "/Users/karimaghimire/Desktop/GOODIMG/pos7data/log_2025-10-29_14-54-25.csv"
# Where to save plots
output_dir = Path("/Users/karimaghimire/Desktop/GOODIMG/pos7data/OUTPUT")
# suite2p plane dir
S2P_DIR = Path("/Users/karimaghimire/Desktop/GOODIMG/pos7data/suite2p/plane0")

# Imaging start time
USE_HUMAN_IMAGING_START = True
imaging_start_human = "2025-05-06 11:29:38.927"  # local human time w/ ms
local_tz = zoneinfo.ZoneInfo("Europe/London")     # your local timezone
# If you prefer to use a known Unix seconds directly, set USE_HUMAN_IMAGING_START=False and fill this:
imaging_start_unix_s_override = None  # e.g., 1714980392.415


# Acquisition
fps = 15.22

# ==================== AUTOMATIC SIGNALS CSV CREATOR ====================
# Uses your defined S2P_DIR to create a Suite2p-derived signals CSV
# (time × cell), keeping only iscell == 1


def create_signals_csv_from_suite2p(s2p_dir: Path, output_csv_path: Path):
    """Creates a signals CSV from Suite2p outputs in the given directory."""
    try:
        # --- Load Suite2p outputs ---
        F = np.load(s2p_dir / "F.npy", allow_pickle=True)
        iscell = np.load(s2p_dir / "iscell.npy", allow_pickle=True)

        # --- Handle iscell shape (can be Nx2 or Nx1) ---
        if iscell.ndim == 2:
            keep_mask = iscell[:, 0].astype(bool)
        else:
            keep_mask = iscell.astype(bool)

        # --- Keep only real cells ---
        F_keep = F[keep_mask]  # shape: (n_cells_kept, n_frames)
        if F_keep.size == 0:
            raise ValueError("No valid cells found (iscell == 1).")

        # --- Transpose to (frames × cells) ---
        Y = F_keep.T

        # --- Create DataFrame with cell columns ---
        Y_df = pd.DataFrame(Y, columns=[f"Y{i+1}" for i in range(Y.shape[1])])

        # --- Save to CSV ---
        output_csv_path.parent.mkdir(parents=True, exist_ok=True)
        Y_df.to_csv(output_csv_path, index=False)

        print(f"[save] Created signals CSV → {output_csv_path}")
        print(f"[info] Shape = {Y.shape[0]} frames × {Y.shape[1]} cells")

    except Exception as e:
        print(f"[error] Failed to create signals CSV: {e}")


# -----------------------------------------------------------------------
# Automatically create the signals CSV if it doesn't exist
signals_csv_path = Path(
    "/Users/karimaghimire/Desktop/GOODIMG/pos7data/suite2p/plane0/F_converted.csv")
S2P_DIR = Path("/Users/karimaghimire/Desktop/GOODIMG/pos7data/suite2p/plane0")

if not signals_csv_path.exists():
    create_signals_csv_from_suite2p(S2P_DIR, signals_csv_path)
else:
    print(f"[info] Found existing signals CSV: {signals_csv_path}")

# ---------------------- ===== PLOTTING & ANALYSIS KNOBS ===== ----------------------

# Figure saving
save_figs = True
show_figs = False

# Toggles
DO_PER_CELL_PLOTS = True   # if True: epoch sequence + averaged traces per best cell
ORDER_HEATMAPS = True      # also save ordered heatmaps
ORDER_METHOD = 'peak_time'  # 'peak_time' or 'activity' (std)

# Analysis choices
N_BEST_EXAMPLE = 6  # run full per-cell pipeline for top 5 cells >>>>>>>>>>>>>>>>>>>>IS THIS RANDOMIZED OR FIXED? !!!!!!!!!!!!!!!!!!!
N_BEST_TUNING = 6   # von Mises tuning panel for top 6 cells
RANDOMIZE_TUNING_PANEL = False
TUNING_RANDOM_SEED = 42  # or None for fresh each run <<??
CLIP_NEGATIVE_SPLINE = True  # clip spline ≥ 0
# ---------------------- ===== AESTHETIC KNOBS (maps) ===== ----------------------
# Dot size sensitivity
# was 95; lower → more dots look big (e.g., 90 or 85)
DOT_REF_PERCENTILE = 90
DOT_SIZE_BASE = 30      # base dot size
DOT_SIZE_GAIN = 100    # scaling of dot size
DOT_SCALE_MODE = 'sqrt'  # 'sqrt' (gentle), 'linear', or 'log'

# Colors
PREF_CMAP = 'twilight_shifted'   # cyclic for angles
OSI_CMAP = 'viridis'            # or 'magma', 'plasma', 'turbo'

# Marker appearance
DOT_ALPHA = 0.95
EDGE_LW = 0.8
WHITE_HALO_LW = 1.6
JITTER_PX = 0.0  # set to ~0.2 if many dots overlap

# Gray for cells with insufficient data
INSUFFICIENT_GRAY = '#C8C8C8'

# Stimulus label→angle mapping (used for tuning + OSI)
label_to_angle = {
    'flicker_1': 0,
    'flicker_2': 45,
    'flicker_3': 90,
    'flicker_4': 135,
    'catch': 135,
}

# Neuropil coefficient (only affects dF/F for pref/OSI maps; raster uses raw F)
NEUCOEFF = 0.7

# -----------------------===== HELPERS =========-----------------------------------------------------------


def ensure_outdir(path: Path):
    path.mkdir(parents=True, exist_ok=True)


def load_signals_keep_y(csv_path: str):
    df = pd.read_csv(csv_path)
    # Try to autoguess Y columns
    ycols = [c for c in df.columns if 'y' in str(c).lower()]
    if not ycols:
        # fallback: assume X1,Y1,X2,Y2,... => take every 2nd col starting at 2nd
        ycols = list(df.columns[1::2])
    if not ycols:
        raise ValueError("No Y columns found in signals CSV.")
    Y = df[ycols].apply(pd.to_numeric, errors='coerce').to_numpy()
    if Y.shape[0] < Y.shape[1]:
        Y = Y.T
    return Y, ycols


def load_stimlog_csv(stim_csv_path: str):
    df = pd.read_csv(stim_csv_path)
    required = ['Stimulus', 'Iteration', 'Start time', 'End time']
    for col in required:
        if col not in df.columns:
            raise ValueError(f"Stim log missing column: {col}")
    return (df['Stimulus'].astype(str).tolist(),
            df['Iteration'].astype(int).tolist(),
            df['Start time'].astype(np.int64).to_numpy(),
            df['End time'].astype(np.int64).to_numpy())


def to_unix_from_human(dt_str: str, tz: timezone) -> float:
    dt = datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S.%f").replace(tzinfo=tz)
    return dt.timestamp()


def dff_epoch_trace(trace_1d: np.ndarray) -> np.ndarray:
    """Robust per-epoch ΔF/F using a positive baseline."""
    if trace_1d.size == 0 or np.all(~np.isfinite(trace_1d)):
        return np.zeros_like(trace_1d, dtype=float)
    p10 = np.nanpercentile(trace_1d, 10)
    F0 = p10 if np.isfinite(p10) and p10 > 0 else np.nanmedian(trace_1d)
    if not np.isfinite(F0) or F0 <= 0:
        F0 = 1e-6
    return (trace_1d - F0) / F0


def stimlog_to_frames(start_ms: np.ndarray, end_ms: np.ndarray, imaging_start_unix_s: float, fs: float, n_time_all: int):
    # Treat as absolute Unix if timestamps are huge (>> experiment duration)
    looks_unix = np.nanmedian(start_ms) > 1e11  # ~ >3.1 years in ms
    if looks_unix:
        offsets_s = (start_ms / 1000.0) - imaging_start_unix_s
        start_fr = np.round(offsets_s * fs).astype(int)
        end_fr = np.round(
            ((end_ms / 1000.0) - imaging_start_unix_s) * fs).astype(int)
        mode = 'absolute_unix_ms'
    else:
        # fallback: relative ms
        offsets_s = start_ms / 1000.0
        start_fr = np.round(offsets_s * fs).astype(int)
        end_fr = np.round((end_ms / 1000.0) * fs).astype(int)
        mode = 'relative_ms'

    # clip to movie bounds
    start_fr = np.clip(start_fr, 0, n_time_all - 1)
    end_fr = np.clip(end_fr,   0, n_time_all)

    # ensure end > start
    for i in range(len(start_fr)):
        if end_fr[i] <= start_fr[i]:
            end_fr[i] = min(n_time_all, start_fr[i] +
                            max(1, int(round(0.5 * fs))))  # 0.5 s fallback

    return start_fr, end_fr, offsets_s, mode


def get_best_cells(Y_all: np.ndarray, epoch_starts: np.ndarray, epoch_ends: np.ndarray, n_best: int = 4):
    """Return indices of n_best neurons with highest peak ΔF/F across epochs."""
    n_cells = Y_all.shape[1]
    peaks = np.zeros(n_cells)
    for c in range(n_cells):
        dff_all = [dff_epoch_trace(Y_all[s:e, c])
                   for s, e in zip(epoch_starts, epoch_ends)]
        valid_peaks = [np.nanmax(ep) for ep in dff_all if ep.size > 0]
        peaks[c] = max(valid_peaks) if valid_peaks else -np.inf
    return np.argsort(peaks)[::-1][:n_best]


def von_mises(x, A, kappa, mu, baseline):
    return A * np.exp(kappa * np.cos(np.deg2rad(x - mu))) / (2*np.pi*i0(kappa)) + baseline


def fit_von_mises(angles, responses):
    try:
        p0 = [np.nanmax(responses), 1.0, angles[int(
            np.nanargmax(responses))], np.nanmin(responses)]
        popt, _ = curve_fit(von_mises, angles, responses, p0=p0, maxfev=5000)
        return popt
    except Exception as e:
        warnings.warn(f"von Mises fit failed: {e}")
        return None


def iqr(x):
    x = np.asarray(x)
    x = x[np.isfinite(x)]
    if x.size == 0:
        return np.nan
    return np.nanpercentile(x, 75) - np.nanpercentile(x, 25)


# --------------------------===== LOAD & ALIGN =======-------------------------------------------------
ensure_outdir(output_dir)

# Imaging start
if USE_HUMAN_IMAGING_START:
    imaging_start_unix_s = to_unix_from_human(imaging_start_human, local_tz)
else:
    if imaging_start_unix_s_override is None:
        raise ValueError(
            "Set imaging_start_unix_s_override if not using human time.")
    imaging_start_unix_s = float(imaging_start_unix_s_override)
print(f"[time] imaging_start_unix_s = {imaging_start_unix_s:.3f}")

# Signals & stim log
Y_all, ycols = load_signals_keep_y(str(signals_csv_path))
n_time_all, n_cells_all = Y_all.shape
print(
    f"[signals] Y shape = {Y_all.shape} (time × cells) — duration ≈ {n_time_all/fps:.2f}s @ {fps} Hz")
stim_labels, iterations, start_ms, end_ms = load_stimlog_csv(
    str(stimlog_csv_path))
print(f"[stimlog] Loaded {len(stim_labels)} epochs")

# Convert to frames (auto absolute vs relative detection)
epoch_starts, epoch_ends, offsets_s, timing_mode = stimlog_to_frames(
    start_ms, end_ms, imaging_start_unix_s, fps, n_time_all)
print("\n=== Timing Info ===")
print(f"Mode: {timing_mode}")
print(f"Imaging start (Unix s): {imaging_start_unix_s:.3f}")
print(f"First stim timestamp (s in log): {start_ms[0]/1000.0:.3f}")
print(
    f"First stim offset vs imaging start: {offsets_s[0]:.3f} s (≈ frame {int(round(offsets_s[0]*fps))})")

# Drop epochs outside movie, if any
valid = (epoch_starts >= 0) & (epoch_starts < n_time_all)
if not np.all(valid):
    bad = np.where(~valid)[0].tolist()
    print(f"[warn] Dropping out-of-range epochs: {bad}")
    epoch_starts = epoch_starts[valid]
    epoch_ends = epoch_ends[valid]
    stim_labels = [lab for k, lab in enumerate(stim_labels) if valid[k]]
    iterations = [it for k, it in enumerate(iterations) if valid[k]]

# ------------------------------===== trials.csv writer  =====------------------------------
trials = pd.DataFrame({
    'trial': iterations,
    'label': stim_labels,
    'ori_deg': [label_to_angle.get(lbl, np.nan) for lbl in stim_labels],
    'start_frame': epoch_starts,
    'end_frame': epoch_ends,
    'start_time_s': epoch_starts / fps,
    'end_time_s':   epoch_ends / fps,
})
ensure_outdir(output_dir)
trials_csv_out = output_dir / 'trials.csv'
trials.to_csv(trials_csv_out, index=False)
print(f"[save] Wrote aligned trials.csv → {trials_csv_out}")

# Optional copy into suite2p dir
if S2P_DIR.exists():
    (S2P_DIR / 'trials.csv').write_text(trials.to_csv(index=False))
    print(f"[save] Wrote trials.csv copy → {S2P_DIR / 'trials.csv'}")
else:
    print(f"[info] S2P_DIR not found → skipping suite2p copy: {S2P_DIR}")

# ------------------------------ ===== Select best cells  ===== ------------------------------
best_idx_4 = get_best_cells(
    Y_all, epoch_starts, epoch_ends, n_best=N_BEST_EXAMPLE)
best_idx_k = get_best_cells(Y_all, epoch_starts, epoch_ends,
                            n_best=max(N_BEST_TUNING, N_BEST_EXAMPLE))

# NOTE: panel_cells gives you the indices of plots (idx + 1), if this looks sketchy, check `get_best_cells` is doing the right thing
# Build panel list (top K), then optionally randomize order for plotting
panel_cells = best_idx_k[:N_BEST_TUNING]
if RANDOMIZE_TUNING_PANEL:
    rng = np.random.default_rng(TUNING_RANDOM_SEED)
    rng.shuffle(panel_cells)
print(f"[selection] Best {N_BEST_EXAMPLE}: {[i+1 for i in best_idx_4]}")
print(
    f"[selection] Tuning panel ({N_BEST_TUNING}, randomized={RANDOMIZE_TUNING_PANEL}): {[i+1 for i in panel_cells]}")


# --------------------------------------------===== Per-cell pipeline for top 4 cells =====---------------------------------------------
if DO_PER_CELL_PLOTS:
    for c in best_idx_4:
        cell_trace = Y_all[:, c]
        raw_epochs = [cell_trace[s:e]
                      for s, e in zip(epoch_starts, epoch_ends)]
        cell_epochs_dff = [dff_epoch_trace(ep) for ep in raw_epochs]

# 1) Epoch sequence — FULL recording with stim markers (dotted) + labels
# # make a continuous ΔF/F for the entire trace to plot against absolute time
        # same baseline method, just once on full trace
        cell_dff_full = dff_epoch_trace(cell_trace)
        t_full = np.arange(len(cell_dff_full)) / fps

        plt.figure(figsize=(12, 4))
        plt.plot(t_full, cell_dff_full, linewidth=1, label='ΔF/F (full)')

        # vertical lines at each epoch start/end (dotted), with labels at starts
        for i, (s, e) in enumerate(zip(epoch_starts, epoch_ends)):
            xs, xe = s / fps, e / fps
            # start line (dotted, darker)
            plt.axvline(xs, linestyle='--', linewidth=0.8,
                        alpha=0.8, color='k')
            # end line (dotted, lighter)
            plt.axvline(xe, linestyle='--', linewidth=0.8,
                        alpha=0.4, color='k')
            # label near the start line
            lbl = stim_labels[i] if i < len(stim_labels) else f'epoch{i+1}'
            # place text slightly above the local baseline to avoid overlap
            plt.text(xs, np.nanmedian(cell_dff_full[max(0, int(s)-10):int(s)+10]) + 0.02,
                     lbl, rotation=90, va='bottom', ha='right', fontsize=8, alpha=0.9)

        plt.xlabel('Time (s)')
        plt.ylabel('ΔF/F')
        plt.title(
            f'Neuron {c+1} — Full Recording with Stimulus Epochs (dotted)')
        plt.xlim(0, n_time_all / fps)
        if save_figs:
            plt.savefig(
                output_dir / f"n{c+1}_epoch_sequence_FULL.png", dpi=150, bbox_inches='tight')
        if show_figs:
            plt.show()
        plt.close()

        # 2) Averaged traces per stimulus label
        avg_by_label, sem_by_label = {}, {}
        for lbl in sorted(set(stim_labels)):
            idxs = [i for i, l in enumerate(stim_labels) if l == lbl]
            if not idxs:
                continue
            traces = [cell_epochs_dff[i] for i in idxs]
            Lmin = min(len(t) for t in traces)
            stack = np.stack([t[:Lmin] for t in traces], axis=0)
            avg_by_label[lbl] = np.nanmean(stack, axis=0)
            sem_by_label[lbl] = np.nanstd(
                stack, axis=0) / np.sqrt(stack.shape[0])

        plt.figure()
        for lbl, avg in avg_by_label.items():
            t_avg = np.arange(len(avg)) / fps
            plt.plot(t_avg, avg, label=lbl)
        plt.xlabel('Time (s)')
        plt.ylabel('ΔF/F (avg)')
        plt.title(f'Neuron {c+1} — Averaged traces')
        plt.legend()
        if save_figs:
            plt.savefig(
                output_dir / f"n{c+1}_avg_traces.png", dpi=150, bbox_inches='tight')
        if show_figs:
            plt.show()
        plt.close()

        # 3) Tuning curve (peaks) with von Mises + spline
        peaks_by_label = {lbl: float(
            np.nanmax(tr)) for lbl, tr in avg_by_label.items() if lbl in label_to_angle}
        if len(peaks_by_label) >= 2:
            # combine labels that map to the same angle (e.g., flicker_4 & catch → 135°)
            angle_bins = {}
            for lbl, pk in peaks_by_label.items():
                ang = int(label_to_angle[lbl])
                angle_bins.setdefault(ang, []).append(pk)
            # average if multiple labels share the angle
            ori_angles_deg = np.array(sorted(angle_bins.keys()), dtype=float)
            ori_responses = np.array([np.nanmean(angle_bins[a])
                                     for a in sorted(angle_bins)], dtype=float)

            # enforce the canonical order on x
            CANON = np.array([0, 45, 90, 135], dtype=float)
            keep_mask = np.isin(ori_angles_deg, CANON)
            ori_angles_deg = ori_angles_deg[keep_mask]
            ori_responses = ori_responses[keep_mask]

            si = np.argsort(ori_angles_deg)
            ori_angles_deg = ori_angles_deg[si]
            ori_responses = ori_responses[si]

            si = np.argsort(ori_angles_deg)
            ori_angles_deg = ori_angles_deg[si]
            ori_responses = ori_responses[si]

            ori_errors = np.array([
                float(np.nanmax(sem_by_label[lbl])
                      ) if lbl in sem_by_label else 0.0
                for lbl in np.array(list(peaks_by_label.keys()))[si]
            ])

            plt.figure()
            plt.errorbar(ori_angles_deg, ori_responses, yerr=ori_errors,
                         fmt='ko', capsize=3, label='data ± SEM')

            popt = fit_von_mises(ori_angles_deg, ori_responses)
            if popt is not None:
                x_fit = np.linspace(0, 180, 200)
                y_fit = von_mises(x_fit, *popt)
                if CLIP_NEGATIVE_SPLINE:
                    y_fit = np.clip(y_fit, 0, None)
                plt.plot(x_fit, y_fit, 'r-', label='von Mises')

            try:
                x_spline = np.linspace(0, 180, 200)
                spl = make_interp_spline(
                    ori_angles_deg, ori_responses, k=min(3, len(ori_angles_deg)-1))
                y_spl = spl(x_spline)
                if CLIP_NEGATIVE_SPLINE:
                    y_spl = np.clip(y_spl, 0, None)
                plt.plot(x_spline, y_spl, 'b--', label='spline')
            except Exception as e:
                warnings.warn(f"Spline failed (cell {c+1}): {e}")

            plt.xlabel('Orientation (°)')
            plt.ylabel('Peak ΔF/F')
            plt.title(f'Neuron {c+1} tuning curve')
            plt.xticks([0, 45, 90, 135], ['0', '45', '90', '135'])

            plt.legend()
            if save_figs:
                plt.savefig(
                    output_dir / f"n{c+1}_tuning_curve.png", dpi=150, bbox_inches='tight')
            if show_figs:
                plt.show()
            plt.close()

print("[debug] Y_all shape (time x cells):", Y_all.shape)
print("[debug] epoch count:", len(epoch_starts))
lens_frames = (epoch_ends - epoch_starts).astype(int)
print("[debug] epoch length stats (frames): min/med/max =",
      int(np.nanmin(lens_frames)), int(np.nanmedian(lens_frames)), int(np.nanmax(lens_frames)))
print("[debug] epoch length stats (s):     min/med/max =",
      np.nanmin(lens_frames)/fps, np.nanmedian(lens_frames)/fps, np.nanmax(lens_frames)/fps)

# How many non-empty epochs per cell after dff? (if many zeros, upstream timing/mode is wrong)


def _nonempty_epochs_for_cell(ci):
    cnt = 0
    for s, e in zip(epoch_starts, epoch_ends):
        ep = dff_epoch_trace(Y_all[s:e, ci])
        if ep.size and np.isfinite(np.nanmax(ep)):
            cnt += 1
    return cnt


nonempty_counts = np.array([_nonempty_epochs_for_cell(ci)
                           for ci in range(Y_all.shape[1])])
print("[debug] non-empty epochs per cell: min/med/max =",
      int(np.min(nonempty_counts)), int(np.median(nonempty_counts)), int(np.max(nonempty_counts)))

# Show who is being picked and why (peak values)


def _peak_of_cell(ci):
    vals = []
    for s, e in zip(epoch_starts, epoch_ends):
        ep = dff_epoch_trace(Y_all[s:e, ci])
        if ep.size:
            vals.append(np.nanmax(ep))
    return float(np.nanmax(vals)) if len(vals) else np.nan


print("[debug] Best-4 (1-based):", [i+1 for i in best_idx_4])
print("[debug] Peaks of Best-4:",
      {int(i): _peak_of_cell(int(i)) for i in best_idx_4})

# Are *many* cells NaN/empty? If yes, the selector can only choose from a small band (e.g., 50–56)
nan_mask = np.array([np.isnan(_peak_of_cell(ci))
                    for ci in range(Y_all.shape[1])])
print("[debug] cells with NaN peak:", int(
    nan_mask.sum()), "out of", Y_all.shape[1])

# ------------------------------------------------- ===== Von Mises panel for best K cells (tuning) == -------------------------------------------------
vm_r2_list = []  # will store dicts: {'cell': int, 'r2': float}

fig_cols = 3
fig_rows = int(np.ceil(N_BEST_TUNING / fig_cols))
fig, axes = plt.subplots(fig_rows, fig_cols, figsize=(
    4*fig_cols, 3.2*fig_rows), squeeze=False)

for k, c in enumerate(panel_cells):
    ax = axes[k//fig_cols][k % fig_cols]

    # Recompute per-label peaks for cell c
    cell_trace = Y_all[:, c]
    raw_epochs = [cell_trace[s:e] for s, e in zip(epoch_starts, epoch_ends)]
    cell_epochs_dff = [dff_epoch_trace(ep) for ep in raw_epochs]

    # Average per label
    avg_by_label = {}
    for lbl in sorted(set(stim_labels)):
        idxs = [i for i, l in enumerate(stim_labels) if l == lbl]
        if not idxs:
            continue
        traces = [cell_epochs_dff[i] for i in idxs]
        Lmin = min(len(t) for t in traces) if traces else 0
        if Lmin <= 0:
            continue
        stack = np.stack([t[:Lmin] for t in traces], axis=0)
        avg_by_label[lbl] = np.nanmean(stack, axis=0)

    # Peak per label, only for labels that map to an angle
    peaks = {lbl: float(np.nanmax(tr))
             for lbl, tr in avg_by_label.items() if lbl in label_to_angle}
    if len(peaks) < 2:
        ax.set_title(f"Cell {c+1}: N/A")
        ax.axis('off')
        continue

    # Combine labels that share the same angle (e.g., 'catch' & 'flicker_4' -> 135°)
    angle_bins = {}
    angv = int(label_to_angle[lbl])
    angle_bins.setdefault(angv, []).append(pk)

    # Build angle/response arrays with canonical ordering and deduplication
    ang = np.array(sorted(angle_bins.keys()), dtype=float)
    rsp = np.array([np.nanmean(angle_bins[a])
                   for a in sorted(angle_bins)], dtype=float)

    # Keep only the canonical 4 orientations, and order them
    CANON = np.array([0, 45, 90, 135], dtype=float)
    keep_mask = np.isin(ang, CANON)
    ang, rsp = ang[keep_mask], rsp[keep_mask]
    si = np.argsort(ang)
    ang, rsp = ang[si], rsp[si]

    # Axes cosmetics
    ax.set_xticks([0, 45, 90, 135])
    ax.set_xticklabels(['0', '45', '90', '135'])

    # Scatter data
    ax.scatter(ang, rsp, c='k', s=18, label='data')

    # Fit von Mises only if we have ≥ 3 distinct points
    popt = None
    if np.unique(ang).size >= 3:
        popt = fit_von_mises(ang, rsp)

    if popt is not None:
        x_fit = np.linspace(0, 180, 200)
        y_fit = von_mises(x_fit, *popt)
        if CLIP_NEGATIVE_SPLINE:
            y_fit = np.clip(y_fit, 0, None)
        ax.plot(x_fit, y_fit, 'r-', lw=1.5, label='von Mises')

        # R^2
        y_pred = von_mises(ang, *popt)
        ss_res = np.nansum((rsp - y_pred)**2)
        ss_tot = np.nansum((rsp - np.nanmean(rsp))**2)
        r2 = 1.0 - ss_res/ss_tot if ss_tot > 0 else np.nan
        vm_r2_list.append({'cell': int(c+1), 'r2': float(r2)})

    # Optional spline (safe with few points by adapting k)
    try:
        xs = np.linspace(0, 180, 200)
        k_spline = min(3, max(1, len(ang)-1))
        if k_spline >= 1 and len(ang) >= 2:
            spl = make_interp_spline(ang, rsp, k=k_spline)
            ys = spl(xs)
            if CLIP_NEGATIVE_SPLINE:
                ys = np.clip(ys, 0, None)
            ax.plot(xs, ys, 'b--', lw=1.2, label='spline')
    except Exception:
        pass

    ax.set_xlabel('°')
    ax.set_ylabel('Peak ΔF/F')
    ax.set_title(f'Cell {c+1}')

# Hide any unused axes if panel_cells < fig_rows*fig_cols
total_slots = fig_rows * fig_cols
if len(panel_cells) < total_slots:
    for kk in range(len(panel_cells), total_slots):
        axes[kk//fig_cols][kk % fig_cols].axis('off')

# Tidy legend (if any)
handles, labels = axes[0][0].get_legend_handles_labels()
if handles:
    fig.legend(handles, labels, loc='upper right')

fig.tight_layout()
if save_figs:
    plt.savefig(output_dir / 'topK_tuning_curves.png',
                dpi=160, bbox_inches='tight')
plt.close(fig)

# --- Save per-cell R² (once, after the loop) and print to stats.txt ---
stats_dir = output_dir / "stats"
ensure_outdir(stats_dir)

vm_r2_df = pd.DataFrame(vm_r2_list)
if not vm_r2_df.empty:
    vm_r2_csv = stats_dir / "tuning_vonmises_r2.csv"
    vm_r2_df.to_csv(vm_r2_csv, index=False)

    lines += ["Top-5 von Mises fit quality (per cell):"]
    if vm_r2_list:
        lines += ["Top-5 von Mises fit quality (per cell):"]
        for row in vm_r2_list:
            lines += [f"  cell {row['cell']}: R² = {row['r2']:.3f}"]
        lines += [""]


# ---------------------------------------===== Heatmaps (signals CSV)====---------------------------------------
# Heatmap: z-score per cell across time (per-row standardization)
# Red = above that cell’s own mean; Blue = below its mean.
resp_z = zscore(Y_all, axis=0, nan_policy='omit').T  # cells × time

# 1) Default order
plt.figure(figsize=(11, 6))
plt.imshow(resp_z, aspect='auto', cmap='RdBu_r', origin='lower',
           extent=[0, n_time_all/fps, 0, resp_z.shape[0]], vmin=-2, vmax=2, interpolation='nearest')
plt.colorbar(label='Activity (z-score per cell)')
plt.xlabel('Time (s)')
plt.ylabel('Neuron #')
plt.title('Heatmap (signals CSV) — z-scored per neuron')
if save_figs:
    plt.savefig(output_dir / 'population_heatmap_zscore_signals.png',
                dpi=160, bbox_inches='tight')
plt.close()

# 2) Save ORDERED version
if ORDER_HEATMAPS:
    if ORDER_METHOD == 'peak_time':
        # frame of each cell’s maximum z
        peak_idx = np.nanargmax(resp_z, axis=1)
        order = np.argsort(peak_idx)                 # sort by that time
    elif ORDER_METHOD == 'activity':
        activity = np.nanstd(resp_z, axis=1)         # active cells first
        order = np.argsort(-activity)
    else:
        order = np.arange(resp_z.shape[0])

    resp_z_ord = resp_z[order]
    plt.figure(figsize=(11, 6))
    plt.imshow(resp_z_ord, aspect='auto', cmap='RdBu_r', origin='lower',
               extent=[0, n_time_all/fps, 0, resp_z_ord.shape[0]], vmin=-0, vmax=10, interpolation='nearest')
    plt.colorbar(label='Activity (z-score per cell)')
    plt.xlabel('Time (s)')
    plt.ylabel('Neuron # (ordered)')
    plt.title(
        f'Heatmap (signals CSV) — ordered by {ORDER_METHOD.replace("_"," ")}')
    if save_figs:
        plt.savefig(output_dir / 'population_heatmap_zscore_signals_ORDERED.png',
                    dpi=160, bbox_inches='tight')
    plt.close()

###!!!!
###!!!!!
# !!!!!
#####!!!!
###!!!!!
# !!!!!

# ==== Suite2p-based maps + rasters (clean version) ====
# • Analysis: ΔF (or ΔF/F if USE_DFF=True) with per-epoch pre-stim baselines
# • Maps: Preferred Orientation (HSV) + OSI (plasma)
# • Rasters: seconds on x-axis, white dotted onset/offset lines, bottom orientation labels
# • Top cell for full-epoch plot: chosen by OSI with an amplitude gate
#
# This block is self-contained: it defines safe defaults if any settings are missing.

# --------------------------- Safe defaults (do not override if already set) ---------------------------
# Check if this knob was already set; if not, use this backup value
AUTO_PRESTIM = globals().get('AUTO_PRESTIM', True)
PRESTIM_SEC = globals().get('PRESTIM_SEC', 1.0)
PRESTIM_SEC_CAP = globals().get('PRESTIM_SEC_CAP', 2.0)
MIN_PRE_FRAMES = globals().get('MIN_PRE_FRAMES', 3)
USE_DFF = globals().get('USE_DFF', False)
FPS = globals().get('FPS', 7.67)

RASTER_ORDER = globals().get('RASTER_ORDER', 'osi')  # 'none'|'prefori'|'amp'|'osi'
# 'row'|'global'|'fixed'|'none'
RASTER_SCALE_MODE = globals().get('RASTER_SCALE_MODE', 'row')
RASTER_GLOBAL_PLOW = globals().get('RASTER_GLOBAL_PLOW', 1.0)
RASTER_GLOBAL_PHIGH = globals().get('RASTER_GLOBAL_PHIGH', 99.0)
RAWF_CBAR_MIN = globals().get('RAWF_CBAR_MIN', None)
RAWF_CBAR_MAX = globals().get('RAWF_CBAR_MAX', None)
SPIKES_CBAR_MIN = globals().get('SPIKES_CBAR_MIN', 0.0)
SPIKES_CBAR_MAX = globals().get('SPIKES_CBAR_MAX', 0.6)
STIM_BOX_ZORDER = globals().get('STIM_BOX_ZORDER', 20)
STIM_BOX_LINEWIDTH = globals().get('STIM_BOX_LINEWIDTH', 1.5)
STIM_BOX_HALO = globals().get('STIM_BOX_HALO', False)

suite2p_stats = globals().get('suite2p_stats', None)

# ---------------------------------------------------------------------
if S2P_DIR.exists():
    try:
        # ---------- Load Suite2p -----------------------------------------------------------------------
        stat = np.load(S2P_DIR / 'stat.npy', allow_pickle=True)
        # raw fluorescence (n_cells x n_frames)
        F = np.load(S2P_DIR / 'F.npy')
        Fneu = np.load(S2P_DIR / 'Fneu.npy')    # neuropil (unused here)
        spks = np.load(S2P_DIR / 'spks.npy')    # deconvolved spikes
        iscell = np.load(S2P_DIR / 'iscell.npy')
        keep = iscell[:, 0].astype(
            bool) if iscell.ndim == 2 else iscell.astype(bool)
        xy = np.array([s['med'][::-1] for s in stat])[keep]  # (y,x)->(x,y)
        rawF_keep = F[keep]
        spks_keep = spks[keep]
        n_cells, n_frames = rawF_keep.shape

        # ---------- Trials (already aligned earlier) ----------
        trials_df = trials.copy().dropna(
            subset=['ori_deg']).sort_values('start_frame')
        oris_deg = np.sort(trials_df['ori_deg'].unique())
        oris_mod = (oris_deg % 180.0)  # [0,180)

        # ---------- Determine pre-stim baseline length per epoch ----------
        if AUTO_PRESTIM:
            prelen_map = {}
            prev_end = 0
            for _, row in trials_df.iterrows():
                s = int(row['start_frame'])
                e = int(row['end_frame'])
                gap_frames = max(0, s - prev_end)
                pre_frames_i = max(MIN_PRE_FRAMES, min(
                    gap_frames, int(round(PRESTIM_SEC_CAP * FPS))))
                prelen_map[(s, e)] = pre_frames_i
                prev_end = e
        else:
            fixed_pre_frames = int(round(PRESTIM_SEC * FPS))

        # ---------- Helper: per-trial response with pre-stim baseline ----------
        def resp_trial_means_for_ori(windows):
            """Return mean response per cell for a list of (start,end) windows."""
            if len(windows) == 0:
                return np.full(n_cells, np.nan, dtype=np.float32)
            out = []
            for (s, e) in windows:
                s, e = int(s), int(e)
                if e <= s or s <= 0 or e > n_frames:
                    continue
                pre_frames_i = prelen_map.get((s, e), int(
                    round(PRESTIM_SEC * FPS))) if AUTO_PRESTIM else fixed_pre_frames
                pre_start = max(s - pre_frames_i, 0)
                pre = rawF_keep[:, pre_start:s]
                if pre.shape[1] >= MIN_PRE_FRAMES and np.all(np.isfinite(pre)):
                    F0 = pre.mean(axis=1)
                else:
                    epoch = rawF_keep[:, s:e]
                    F0 = np.nanmedian(epoch, axis=1)
                F0 = np.where(np.isfinite(F0) & (F0 > 0), F0,
                              np.nanmedian(rawF_keep, axis=1))
                stim = rawF_keep[:, s:e]
                stim_mean = np.nanmean(stim, axis=1)
                resp_vec = (stim_mean - F0) / \
                    F0 if USE_DFF else (stim_mean - F0)
                out.append(resp_vec)
            if not out:
                return np.full(n_cells, np.nan, dtype=np.float32)
            return np.nanmean(np.vstack(out), axis=0).astype(np.float32)

        # ---------- Windows per orientation ----------
        ori_windows = []
        for ori in oris_deg:
            sub = trials_df.loc[trials_df['ori_deg'] == ori, [
                'start_frame', 'end_frame']].to_numpy(int)
            sub = sub[(sub[:, 1] - sub[:, 0]) >= 5]
            ori_windows.append([tuple(x) for x in sub])

        # ---------- Response matrix (cells x orientations) ----------
        resp = np.zeros((n_cells, len(oris_deg)), dtype=np.float32)
        for j, windows in enumerate(ori_windows):
            resp[:, j] = resp_trial_means_for_ori(windows)

        # ---------- Preferred orientation & classical OSI ----------
        pref_idx = np.nanargmax(resp, axis=1)
        pref_ori = oris_mod[pref_idx]  # [0,180)

        def circdist180(a, b):
            d = np.abs(a - b) % 180.0
            return np.minimum(d, 180.0 - d)
        orth_idx = np.array([int(np.argmin(
            np.abs(circdist180(oris_mod, po) - 90.0))) for po in pref_ori], dtype=int)

        rows = np.arange(n_cells)
        r_pref = np.maximum(resp[rows, pref_idx], 0.0)
        r_orth = np.maximum(resp[rows, orth_idx], 0.0)
        den = r_pref + r_orth
        osi = np.full_like(r_pref, np.nan, dtype=np.float32)
        ok_den = den > 0
        osi[ok_den] = (r_pref[ok_den] - r_orth[ok_den]) / den[ok_den]
        amp = r_pref.copy()  # dot size

        # ---- SAVE per-cell metrics (Suite2p) ----
        suite2p_metrics = pd.DataFrame({
            'cell': np.arange(1, n_cells+1, dtype=int),
            'pref_deg': pref_ori.astype(float),
            'osi': osi.astype(float),
            'amp': amp.astype(float)
        })
        suite2p_metrics_out = output_dir / 'suite2p_cell_metrics.csv'
        suite2p_metrics.to_csv(suite2p_metrics_out, index=False)

        # ---------- Masks for plotting ----------
        finite_counts = np.sum(np.isfinite(resp), axis=1)
        ok = finite_counts >= 1
        amp_plot = np.where(ok, amp, 0.0)
        pref_plot = np.where(ok, pref_ori, np.nan)
        osi_plot = np.where(ok, osi, np.nan)

        # ---------- Preferred Orientation Map --------------------------------------------------
        plt.figure(figsize=(6.5, 6.5), facecolor='w')
        if np.any(ok):
            sc = plt.scatter(xy[ok, 0], xy[ok, 1],
                             c=pref_plot[ok], cmap='hsv', vmin=0, vmax=180,
                             s=np.clip(amp_plot[ok], 0, None) * 1.0, alpha=0.9, edgecolors='k')
            cb = plt.colorbar(sc, label='Preferred orientation (deg)')
            ticks = np.unique(np.round((oris_mod % 180.0)).astype(int))
            cb.set_ticks(ticks)
            cb.set_ticklabels([f'{t}°' for t in ticks])
        plt.gca().invert_yaxis()
        plt.axis('equal')
        plt.xlabel('X (px)')
        plt.ylabel('Y (px)')
        plt.title('Preferred Orientation (color), Amplitude (size)')
        if save_figs:
            plt.savefig(output_dir / 'pref_orientation_map.png', dpi=220)
        plt.close()

        # ---------- OSI Map ----------
        plt.figure(figsize=(6.5, 6.5), facecolor='w')
        if np.any(ok):
            sc2 = plt.scatter(xy[ok, 0], xy[ok, 1],
                              c=osi_plot[ok], cmap='plasma', vmin=0, vmax=1,
                              s=np.clip(amp_plot[ok], 0, None) * 1.5, alpha=0.9, edgecolors='k')
            plt.colorbar(sc2, label='OSI (0–1)')
        plt.gca().invert_yaxis()
        plt.axis('equal')
        plt.xlabel('X (px)')
        plt.ylabel('Y (px)')
        plt.title('OSI (color), Amplitude (size)')
        if save_figs:
            plt.savefig(output_dir / 'osi_map.png', dpi=220)
        plt.close()

        # ---------- Raster ordering (visual only) ----------
        ORDER_LABEL = {'none': 'row order', 'prefori': 'preferred orientation',
                       'amp': 'amplitude', 'osi': 'OSI'}.get(RASTER_ORDER, 'row order')
        if RASTER_ORDER == 'prefori':
            order_idx = np.argsort(
                np.where(np.isfinite(pref_plot), pref_plot, 1e9))
        elif RASTER_ORDER == 'amp':
            order_idx = np.argsort(-np.where(np.isfinite(amp_plot),
                                   amp_plot, -np.inf))
        elif RASTER_ORDER == 'osi':
            order_idx = np.argsort(-np.where(np.isfinite(osi_plot),
                                   osi_plot, -np.inf))
        else:
            order_idx = np.arange(n_cells)

        rawF_ord = rawF_keep[order_idx]
        spks_ord = spks_keep[order_idx]

        # ---------- helpers ----------

        def _robust_minmax_rows(mat, p_low=1.0, p_high=99.0):
            lo = np.nanpercentile(mat, p_low, axis=1, keepdims=True)
            hi = np.nanpercentile(mat, p_high, axis=1, keepdims=True)
            scale = np.maximum(hi - lo, 1e-9)
            return np.clip((mat - lo) / scale, 0.0, 1.0)

        # map 0..180° → HSV color (matches pref map)
        def angle_to_color(ang):
            cmap = plt.get_cmap('hsv')
            return cmap((float(ang) % 180.0) / 180.0)

        ###
        ###
        #

        # ---------- RAW-F Raster (seconds axis + white dotted onset/offset) ----------------------------------
        fig, ax = plt.subplots(figsize=(12, 6), facecolor='w')
        T_end = rawF_ord.shape[1] / FPS
        if RASTER_SCALE_MODE == 'row':
            data = _robust_minmax_rows(rawF_ord)
            vmin, vmax = 0.0, 1.0
            label = 'raw F (row-scaled 1–99% → 0–1)'
        elif RASTER_SCALE_MODE == 'fixed':
            if RAWF_CBAR_MIN is None or RAWF_CBAR_MAX is None:
                raise ValueError(
                    "Set RAWF_CBAR_MIN and RAWF_CBAR_MAX when RASTER_SCALE_MODE='fixed'.")
            data = rawF_ord
            vmin, vmax = float(RAWF_CBAR_MIN), float(RAWF_CBAR_MAX)
            label = f'raw F (fixed {RAWF_CBAR_MIN}–{RAWF_CBAR_MAX})'
        elif RASTER_SCALE_MODE == 'global':
            finite = rawF_ord[np.isfinite(rawF_ord)]
            if finite.size == 0:
                vmin, vmax = 0.0, 1.0
            else:
                vmin = np.percentile(finite, RASTER_GLOBAL_PLOW)
                vmax = np.percentile(finite, RASTER_GLOBAL_PHIGH)
                if not np.isfinite(vmin) or not np.isfinite(vmax) or vmax <= vmin:
                    vmin, vmax = float(np.nanmin(finite)), float(
                        np.nanmax(finite))
            data = rawF_ord
            label = f'raw F (global {RASTER_GLOBAL_PLOW}–{RASTER_GLOBAL_PHIGH}th pct)'
        else:
            data = rawF_ord
            vmin = vmax = None
            label = 'raw F (a.u.)'

        im = ax.imshow(data, aspect='auto', interpolation='nearest', origin='lower', cmap='viridis',
                       vmin=vmin, vmax=vmax, extent=(0, T_end, -0.5, rawF_ord.shape[0]-0.5))
        plt.colorbar(im, ax=ax, label=label)
        ax.set_xlabel('Time (s)')
        ax.set_ylabel(f'Cell (kept, ordered by {ORDER_LABEL})')
        ax.set_title('Raw F Raster with stimulus (white dotted)')

        import matplotlib.patheffects as pe
        import matplotlib.transforms as mtransforms
        for _, row in trials_df.iterrows():
            s = int(row['start_frame'])
            e = int(row['end_frame'])
            if e <= s:
                continue
            for x in (s, e):
                x_sec = x / FPS
                ln = ax.axvline(x_sec, color='w', linestyle=':',
                                linewidth=1.5, zorder=STIM_BOX_ZORDER)
                if STIM_BOX_HALO:
                    ln.set_path_effects(
                        [pe.withStroke(linewidth=STIM_BOX_LINEWIDTH+1.2, foreground='k')])
        trans = mtransforms.blended_transform_factory(
            ax.transData, ax.transAxes)

        last_x = -1e9
        min_gap_s = 20.0 / FPS
        for _, row in trials_df.iterrows():
            s = int(row['start_frame'])
            e = int(row['end_frame'])
            if e <= s:
                continue
            mid_s = 0.5 * ((s / FPS) + (e / FPS))
            if mid_s - last_x < min_gap_s:
                continue
            ax.text(mid_s, -0.12, f"{int(row['ori_deg'])}°", transform=trans,
                    ha='center', va='top', fontsize=8, color='w', zorder=STIM_BOX_ZORDER+1)
            last_x = mid_s
        plt.subplots_adjust(bottom=0.2)
        if save_figs:
            plt.savefig(output_dir / 'raster_rawF_dotted_overlay.png',
                        dpi=220, bbox_inches='tight')
        plt.close(fig)

        overlay_angles = trials_df['ori_deg'].to_numpy(float)
        for s, e, ang in zip(trials_df['start_frame'], trials_df['end_frame'], overlay_angles):
            t0, t1 = s / FPS, e / FPS
            bar_color = angle_to_color(float(ang))
            ax.axvspan(t0, t1, color=bar_color, alpha=0.18)

        import matplotlib.patches as mpatches
        legend_angles = np.unique(overlay_angles).astype(int).tolist()
        legend_handles = [mpatches.Patch(color=angle_to_color(
            a), label=f'{a}°') for a in legend_angles]
        ax.legend(handles=legend_handles, loc='upper right',
                  frameon=True, fontsize=8)
        # === END ADD ===

        # ---------- Spikes Raster (seconds axis + white dotted onset/offset) ----------
        if spks_ord is not None and spks_ord.size:
            fig2, ax2 = plt.subplots(figsize=(12, 6), facecolor='w')
            T_end2 = spks_ord.shape[1] / FPS
            im2 = ax2.imshow(spks_ord, aspect='auto', interpolation='nearest', origin='lower', cmap='viridis',
                             vmin=SPIKES_CBAR_MIN, vmax=SPIKES_CBAR_MAX,
                             extent=(0, T_end2, -0.5, spks_ord.shape[0]-0.5))
            plt.colorbar(
                im2, ax=ax2, label=f'spikes ({SPIKES_CBAR_MIN}–{SPIKES_CBAR_MAX})')
            ax2.set_xlabel('Time (s)')
            ax2.set_ylabel(f'Cell (kept, ordered by {ORDER_LABEL})')
            ax2.set_title('Spikes Raster with stimulus (white dotted)')

            for _, row in trials_df.iterrows():
                s = int(row['start_frame'])
                e = int(row['end_frame'])
                if e <= s:
                    continue
                for x in (s, e):
                    x_sec = x / FPS
                    ln = ax2.axvline(x_sec, color='w', linestyle=':',
                                     linewidth=1.5, zorder=STIM_BOX_ZORDER)
                    if STIM_BOX_HALO:
                        ln.set_path_effects(
                            [pe.withStroke(linewidth=STIM_BOX_LINEWIDTH+1.2, foreground='k')])
            trans2 = mtransforms.blended_transform_factory(
                ax2.transData, ax2.transAxes)
            last_x = -1e9
            min_gap_s = 20.0 / FPS
            for _, row in trials_df.iterrows():
                s = int(row['start_frame'])
                e = int(row['end_frame'])
                if e <= s:
                    continue
                mid_s = 0.5 * ((s / FPS) + (e / FPS))
                if mid_s - last_x < min_gap_s:
                    continue
                ax2.text(mid_s, -0.12, f"{int(row['ori_deg'])}°", transform=trans2,
                         ha='center', va='top', fontsize=8, color='w', zorder=STIM_BOX_ZORDER+1)
                last_x = mid_s
            plt.subplots_adjust(bottom=0.2)
            if save_figs:
                plt.savefig(
                    output_dir / 'raster_spikes_dotted_overlay.png', dpi=220, bbox_inches='tight')
            plt.close(fig2)

    except Exception as e:
        warnings.warn(f"Suite2p section skipped due to error: {e}")
else:
    print(
        f"[info] Suite2p directory not found; skipped salt-and-pepper maps: {S2P_DIR}")


###!!!!!
# !!!!!
#####!!!!
###!!!!!
# !!!!!

# ---------------------------------------- ===== Population OSI + angle histos  ==== ----------------------------------------
signals_metrics_rows = []  # will store dicts per cell

pop_pref_angles, pop_osi = [], []
_epoch_pairs = list(zip(epoch_starts, epoch_ends))
for c in range(n_cells_all):
    per_label_traces = {lbl: [] for lbl in label_to_angle}
    for ep_idx, (s, e) in enumerate(_epoch_pairs):
        lbl = str(stim_labels[ep_idx])
        if lbl not in label_to_angle:
            continue
        tr = Y_all[s:e, c]
        dff = dff_epoch_trace(tr)
        per_label_traces[lbl].append(dff)
    peaks = {}
    for lbl, lst in per_label_traces.items():
        if len(lst) == 0:
            continue
        Lmin = min(len(t) for t in lst)
        if Lmin <= 0:
            continue
        stack = np.stack([t[:Lmin] for t in lst], axis=0)
        avg = np.nanmean(stack, axis=0)
        pk = float(np.nanmax(avg))
        peaks[lbl] = max(0.0, pk)  # rectify
    usable = [lbl for lbl in peaks if lbl in label_to_angle]
    if len(usable) < 2:
        continue
    ang = np.array([label_to_angle[lbl] for lbl in usable], dtype=float)
    rsp = np.array([peaks[lbl] for lbl in usable], dtype=float)
    pref_lbl = usable[int(np.argmax(rsp))]
    pop_pref_angles.append(label_to_angle[pref_lbl])
    i_pref = int(np.argmax(rsp))
    pref_angle = ang[i_pref]
    target_orth = (pref_angle + 90.0) % 180.0
    i_orth = int(np.argmin(np.abs(((ang - target_orth + 90) % 180) - 90)))
    R_pref = float(rsp[i_pref])
    R_orth = float(rsp[i_orth])
    osi_val = (R_pref - R_orth) / (R_pref + R_orth + 1e-12)
    # record per-cell metrics (signals CSV basis)
    signals_metrics_rows.append({
        'cell': int(c+1),
        'pref_deg': float(pref_angle),
        'osi': float(osi_val)
    })
    pop_osi.append(osi_val)

# ---- SAVE per-cell metrics (signals CSV) ----
if signals_metrics_rows:
    signals_metrics = pd.DataFrame(signals_metrics_rows)
    signals_metrics_out = output_dir / 'signals_cell_metrics.csv'
    signals_metrics.to_csv(signals_metrics_out, index=False)


def half_polar_rose(angles_deg, weights=None, title='', fname='rose.png'):
    # bin to 0–180 in 15° bins (or choose 30° if you prefer)
    bins = np.arange(0, 181, 15)
    hist, edges = np.histogram(angles_deg, bins=bins, weights=weights)
    centers = np.deg2rad(0.5*(edges[:-1] + edges[1:]))

    fig = plt.figure(figsize=(5, 4))
    ax = fig.add_subplot(111, projection='polar')
    # mirror to 0–pi (half-polar)
    width = np.deg2rad(np.diff(edges))
    ax.bar(centers, hist, width=width, bottom=0.0, align='center',
           edgecolor='k', linewidth=0.5, alpha=0.8)
    ax.set_theta_zero_location('N')  # 0° at top
    ax.set_theta_direction(-1)       # clockwise
    ax.set_thetamin(0)
    ax.set_thetamax(180)
    ax.set_title(title)
    if save_figs:
        plt.savefig(output_dir / fname, dpi=200, bbox_inches='tight')
    plt.close(fig)


# Use signals-based metrics for population rose plots:
if signals_metrics_rows:
    angs = np.array([row['pref_deg']
                    for row in signals_metrics_rows], dtype=float)
    osis = np.array([row['osi'] for row in signals_metrics_rows], dtype=float)

    half_polar_rose(
        angs, None, 'Orientation preference (counts)', 'rose_counts.png')
    half_polar_rose(
        angs, osis, 'Orientation preference (OSI-weighted)', 'rose_osiweighted.png')

# Histograms
if len(pop_pref_angles) > 0:
    ordered_angles = sorted(set(label_to_angle.values()))
    counts = [sum(1 for a in pop_pref_angles if a == A)
              for A in ordered_angles]
    plt.figure()
    plt.bar([str(int(A)) for A in ordered_angles], counts)
    plt.xlabel('Preferred orientation (°)')
    plt.ylabel('Cell count')
    plt.title('Population: preferred orientation distribution')
    if save_figs:
        plt.savefig(output_dir / 'population_pref_orientation_hist.png',
                    dpi=160, bbox_inches='tight')
    plt.close()
if len(pop_osi) > 0:
    plt.figure()
    plt.hist(pop_osi, bins=15, range=(0, 1))
    plt.xlabel('OSI')
    plt.ylabel('Cell count')
    plt.title('Population: OSI distribution')
    if save_figs:
        plt.savefig(output_dir / 'population_osi_hist.png',
                    dpi=160, bbox_inches='tight')
    plt.close()

# ------------------------- ===== stats.txt writer === -------------------------
stats_dir = output_dir / "stats"
ensure_outdir(stats_dir)
lines = []
# Session summary
lines += [
    "Session summary",
    f"  Frames: {n_time_all}",
    f"  Duration (s): {n_time_all/fps:.2f}",
    f"  FPS: {fps}",
    f"  #Cells (signals CSV): {n_cells_all}",
    f"  #Epochs (valid): {len(epoch_starts)}",
    f"  Timing mode: {timing_mode}",
    f"  First stim offset vs imaging start (s): {offsets_s[0]:.3f}",
    ""
]

# Signals CSV quality: per-cell SNR (stim std / baseline std)
stim_mask = np.zeros(n_time_all, dtype=bool)
for s, e in zip(epoch_starts, epoch_ends):
    stim_mask[s:e] = True
base_mask = ~stim_mask
eps = 1e-9
snr_list = []
for c in range(n_cells_all):
    stim_std = np.nanstd(Y_all[stim_mask, c]) if stim_mask.any() else np.nan
    base_std = np.nanstd(Y_all[base_mask, c]) if base_mask.any() else np.nan
    snr = stim_std / \
        (base_std + eps) if np.isfinite(stim_std) and np.isfinite(base_std) else np.nan
    snr_list.append(snr)
snr_arr = np.array(snr_list, dtype=float)

lines += [
    "Signals CSV — trace quality (SNR = std_stim / std_base)",
    f"  median SNR: {np.nanmedian(snr_arr):.3f}",
    f"  IQR SNR: {iqr(snr_arr):.3f}",
    f"  fraction SNR ≥ 1.0: {np.nanmean(snr_arr >= 1.0):.3f}",
    ""
]

# Population tuning (signals CSV)
pop_osi_arr = np.array(pop_osi, dtype=float)
lines += [
    "Population tuning (signals CSV)",
    f"  OSI count: {np.sum(np.isfinite(pop_osi_arr))}",
    f"  OSI median: {np.nanmedian(pop_osi_arr):.3f}",
    f"  OSI IQR: {iqr(pop_osi_arr):.3f}",
    f"  fraction tuned (OSI ≥ 0.30): {np.nanmean(pop_osi_arr >= 0.30):.3f}",
]
# Preferred orientation counts
po_counts = {}
for A in sorted(set(label_to_angle.values())):
    po_counts[int(A)] = int(sum(1 for a in pop_pref_angles if a == A))
lines.append("  preferred orientation counts: " +
             ", ".join([f"{k}°={v}" for k, v in po_counts.items()]))
lines.append("")

# Suite2p stats (if available)
if suite2p_stats is not None:
    amp = suite2p_stats["amp"]
    osi_s2p = suite2p_stats["osi"]
    lines += [
        "Suite2p (iscell) summary",
        f"  total ROIs: {suite2p_stats['n_total_rois']}",
        f"  kept (iscell=1): {suite2p_stats['n_keep']}  ({suite2p_stats['keep_frac']*100:.1f}%)",
        f"  amplitude (pref response) — median: {np.nanmedian(amp):.4f}, IQR: {iqr(amp):.4f}, p95: {np.nanpercentile(amp,95):.4f}",
        f"  OSI — count: {np.sum(np.isfinite(osi_s2p))}, median: {np.nanmedian(osi_s2p):.3f}, IQR: {iqr(osi_s2p):.3f}, fraction tuned (≥0.30): {np.nanmean(osi_s2p >= 0.30):.3f}",
        ""
    ]
else:
    lines += ["Suite2p (iscell) summary",
              "  (no suite2p directory found or section failed)", ""]

# Top-5 von Mises fit quality (R²)
vm_r2_arr = np.array(vm_r2_list, dtype=float)
lines += [
    "Top-5 von Mises fit quality (signals-based tuning)",
    f"  R² count: {np.sum(np.isfinite(vm_r2_arr))}",
    f"  R² median: {np.nanmedian(vm_r2_arr):.3f}",
    f"  R² IQR: {iqr(vm_r2_arr):.3f}",
    ""
]

(stats_dir / "stats.txt").write_text("\n".join(lines) + "\n")
print(f"[save] Wrote stats → {stats_dir / 'stats.txt'}")
print("\nAll done. Figures & trials.csv saved to:", output_dir)
